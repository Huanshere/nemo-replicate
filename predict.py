import os
import gc
import re
import time
import uuid
from typing import List, Tuple, Dict

import torch
import numpy as np
import pandas as pd
import nemo.collections.asr as nemo_asr
import torchaudio
from pydub import AudioSegment
from pyannote.audio import Pipeline
import webrtcvad
from cog import BasePredictor, Input, Path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# VAD / é•¿éŸ³é¢‘åˆ†å—å‚æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VAD_AGGR = 3                # WebRTC VAD æ¿€è¿›åº¦ (0â€‘3)
FRAME_MS = 30               # VAD å¸§é•¿ (æ¯«ç§’)
MIN_SPEECH_MS = 500         # è¿‡æ»¤çŸ­è¯­éŸ³é˜ˆå€¼ (æ¯«ç§’)
MERGE_GAP_SEC = 2.0         # åˆå¹¶ç›¸é‚»è¯­éŸ³æ®µæœ€å¤§é—´éš” (ç§’)

class Predictor(BasePredictor):
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åˆå§‹åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def setup(self):
        """ä»…åœ¨å®¹å™¨å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # â”€ ASR æ¨¡å‹ â”€
        self.asr_model = (
            nemo_asr.models.ASRModel.from_pretrained(
                model_name="nvidia/parakeet-tdt-0.6b-v2"
            )
            .eval()
            .to(self.device)
        )
        # é•¿éŸ³é¢‘ä¼˜åŒ–ï¼šå±€éƒ¨æ³¨æ„åŠ› + å…³é—­ä¸‹é‡‡æ ·åˆ†å—
        self.asr_model.change_attention_model("rel_pos_local_attn", [256, 256])
        self.asr_model.change_subsampling_conv_chunking_factor(1)

        # â”€ Diarization (lazy) â”€
        self.dia_model = None  # å»¶è¿ŸåŠ è½½

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ éŸ³é¢‘é¢„å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @staticmethod
    def _preprocess_audio(audio_path: str) -> Tuple[str, AudioSegment]:
        """è½¬å•å£°é“ / 16â€‘kHz / 16â€‘bit PCM WAV"""
        audio = AudioSegment.from_file(audio_path)
        # ç¡®ä¿é‡‡æ ·ç‡/å£°é“
        audio = audio.set_channels(1).set_frame_rate(16000)
        wav_path = "temp_full_{}.wav".format(uuid.uuid4().hex[:8])
        audio.export(wav_path, format="wav")
        return wav_path, audio

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ VAD è¾…åŠ©å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @staticmethod
    def _detect_speech_ranges(audio: AudioSegment) -> List[Tuple[float, float]]:
        """ä½¿ç”¨ WebRTC VAD æ£€æµ‹è¯­éŸ³åŒºæ®µï¼Œè¿”å› [(start, end) ç§’]"""
        vad = webrtcvad.Vad(VAD_AGGR)
        raw = audio.raw_data
        bytes_per_frame = int(16000 * 2 * FRAME_MS / 1000)
        n_frames = len(raw) // bytes_per_frame

        ranges: List[Tuple[float, float]] = []
        rng_start = None
        for i in range(n_frames):
            frame = raw[i * bytes_per_frame : (i + 1) * bytes_per_frame]
            ts = i * FRAME_MS / 1000.0
            if vad.is_speech(frame, 16000):
                if rng_start is None:
                    rng_start = ts
            else:
                if rng_start is not None:
                    ranges.append((rng_start, ts))
                    rng_start = None
        if rng_start is not None:
            ranges.append((rng_start, len(audio) / 1000.0))

        # è¿‡æ»¤è¿‡çŸ­æ®µ
        ranges = [(s, e) for s, e in ranges if (e - s) * 1000 >= MIN_SPEECH_MS]
        # åˆå¹¶ç›¸é‚»æ®µ
        merged: List[Tuple[float, float]] = []
        for s, e in ranges:
            if not merged or s - merged[-1][1] > MERGE_GAP_SEC:
                merged.append([s, e])
            else:
                merged[-1][1] = e
        return [(float(s), float(e)) for s, e in merged]

    @staticmethod
    def _build_blocks(ranges: List[Tuple[float, float]], max_len: int) -> List[Tuple[float, float]]:
        """æŒ‰ max_len(ç§’) å°†è¯­éŸ³æ®µç»„åˆä¸ºæ›´å¤§å—"""
        if not ranges:
            return []
        blocks = []
        cur_s, cur_e = ranges[0]
        for s, e in ranges[1:]:
            if e - cur_s <= max_len:
                cur_e = e
            else:
                blocks.append((cur_s, cur_e))
                cur_s, cur_e = s, e
        blocks.append((cur_s, cur_e))
        return blocks

    @staticmethod
    def _export_blocks(audio: AudioSegment, blocks: List[Tuple[float, float]]) -> List[Tuple[str, float]]:
        """å°†æ¯ä¸ªå—å¯¼å‡ºä¸ºä¸´æ—¶ WAVï¼Œè¿”å› [(wav_path, offset_sec)]"""
        meta: List[Tuple[str, float]] = []
        for idx, (s, e) in enumerate(blocks):
            clip = audio[int(s * 1000) : int(e * 1000)]
            fname = f"temp_block_{idx}_{uuid.uuid4().hex[:6]}.wav"
            clip.export(fname, format="wav")
            meta.append((fname, s))
        return meta

    def _vad_chunk(self, audio: AudioSegment, max_seg_sec: int) -> List[Tuple[str, float]]:
        ranges = self._detect_speech_ranges(audio)
        blocks = self._build_blocks(ranges, max_seg_sec)
        print(f"âœ‚ï¸  VAD å‘ç° {len(ranges)} ä¸ªè¯­éŸ³æ®µï¼Œåˆå¹¶ä¸º {len(blocks)} ä¸ªå— â€¦")
        return self._export_blocks(audio, blocks) if blocks else []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Diarization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _load_diarization_pipeline(self):
        if self.dia_model is None:
            self.dia_model = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token="hf_lwgY" + "yWdYzpKayYQitZLLceYAoPGZpnYdzT",
            ).to(self.device)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸»è¦å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def predict(
        self,
        audio: Path = Input(description="è¾“å…¥éŸ³é¢‘æ–‡ä»¶ (mp3/wav)"),
        max_segment_minutes: int = Input(description="å•å—è¯­éŸ³æœ€å¤§åˆ†é’Ÿæ•°", default=20),
        return_word_timestamps: bool = Input(description="æ˜¯å¦è¿”å›åˆ†è¯æ—¶é—´æˆ³", default=True),
        speaker_diarization: bool = Input(description="æ˜¯å¦æ‰§è¡Œè¯´è¯äººåˆ†ç¦»", default=False),
        min_speakers: int = Input(description="æœ€å°‘è¯´è¯äººï¼Œ-1 è¡¨ç¤ºä¸æŒ‡å®š", default=-1),
        max_speakers: int = Input(description="æœ€å¤šè¯´è¯äººï¼Œ-1 è¡¨ç¤ºä¸æŒ‡å®š", default=-1),
    ) -> Dict:

        # â”€ 1. é¢„å¤„ç† â”€
        wav_path, audio_seg = self._preprocess_audio(str(audio))
        duration_sec = len(audio_seg) / 1000.0
        
        # è®¡ç®—æœ€å¤§åˆ†æ®µç§’æ•°
        max_seg_sec = max_segment_minutes * 60

        # â”€ 2. VAD åˆ‡å— â”€
        blocks = self._vad_chunk(audio_seg, max_seg_sec)
        # è‹¥æœªæ£€æµ‹åˆ°è¯­éŸ³åˆ™ä½¿ç”¨æ•´å—
        if not blocks:
            blocks = [(wav_path, 0.0)]

        segments_asr: List[Dict] = []
        word_ts_all: List[Dict] = []

        # â”€ 3. ASR â”€
        t0 = time.time()
        for wav_block, offset in blocks:
            print(f"ğŸ“ è½¬å†™å— {wav_block} (offset={offset:.1f}s)â€¦")
            try:
                hyps = self.asr_model.transcribe([wav_block], timestamps=True, batch_size=1)
            except ValueError:
                hyps = self.asr_model.transcribe([wav_block], batch_size=1)
            hyp = hyps[0]

            # â”€ 3â€‘a å¥å­çº§æ—¶é—´æˆ³ â”€
            seg_list = hyp.timestamp.get("segment", [])
            if not seg_list:  # éƒ¨åˆ†æ¨¡å‹æœªè¿”å› segment
                duration_block = AudioSegment.from_file(wav_block).duration_seconds
                seg_list = [{"start": 0.0, "end": duration_block, "segment": hyp.text}]
            for seg in seg_list:
                if not seg["segment"].strip():
                    continue
                segments_asr.append(
                    {
                        "start": offset + seg["start"],
                        "end": offset + seg["end"],
                        "text": seg["segment"],
                    }
                )

            # â”€ 3â€‘b åˆ†è¯çº§æ—¶é—´æˆ³ â”€
            if return_word_timestamps and "word" in hyp.timestamp:
                for w in hyp.timestamp["word"]:
                    word_ts_all.append(
                        {
                            "word": w["word"],
                            "start": offset + w["start"],
                            "end": offset + w["end"],
                        }
                    )

            # æ¸…ç†å—æ–‡ä»¶
            if wav_block != wav_path and os.path.exists(wav_block):
                os.remove(wav_block)

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        print(f"â±ï¸ ASR æ€»è€—æ—¶ï¼š{time.time() - t0:.2f}s")

        # æŒ‰æ—¶é—´æ’åº
        segments_asr.sort(key=lambda x: x["start"])

        # â”€ 4. Speaker Diarization â”€
        if speaker_diarization and segments_asr:
            self._load_diarization_pipeline()

            waveform, sr = torchaudio.load(wav_path)
            diar_kwargs = {}
            if min_speakers > 0:
                diar_kwargs["min_speakers"] = min_speakers
            if max_speakers > 0:
                diar_kwargs["max_speakers"] = max_speakers

            dia_result = self.dia_model({"waveform": waveform, "sample_rate": sr}, **diar_kwargs)
            dia_df = pd.DataFrame(
                [
                    {"start": turn.start, "end": turn.end, "speaker": spk}
                    for turn, _, spk in dia_result.itertracks(yield_label=True)
                ]
            )

            # 4â€‘a å¯¹é½è¯´è¯äººåˆ°ç‰‡æ®µ
            for seg in segments_asr:
                dia_df["overlap"] = np.minimum(dia_df["end"], seg["end"]) - np.maximum(
                    dia_df["start"], seg["start"]
                )
                overlap = dia_df[dia_df["overlap"] > 0]
                seg["speaker"] = (
                    overlap.groupby("speaker")["overlap"].sum().idxmax() if not overlap.empty else "UNKNOWN"
                )

            # 4â€‘b åˆå¹¶åŒè¯´è¯äººä¸”è¿ç»­ç‰‡æ®µ
            merged: List[Dict] = []
            SENT_END = re.compile(r"[.!?]+$")
            cur = segments_asr[0].copy()
            for nxt in segments_asr[1:]:
                cond_same_spk = nxt.get("speaker") == cur.get("speaker")
                cond_gap = nxt["start"] - cur["end"] <= 1.0
                cond_len = (cur["end"] - cur["start"]) < 30.0
                cond_punc = not SENT_END.search(cur["text"][-1:])
                if cond_same_spk and cond_gap and cond_len and cond_punc:
                    cur["end"] = nxt["end"]
                    cur["text"] += " " + nxt["text"]
                else:
                    merged.append(cur)
                    cur = nxt.copy()
            merged.append(cur)
            segments_final = merged
        else:
            segments_final = segments_asr

        # â”€ 5. æ„å»ºç»“æœ â”€
        result: Dict = {"duration_seconds": duration_sec, "segments": segments_final}
        if return_word_timestamps and word_ts_all:
            result["word_timestamps"] = word_ts_all

        # â”€ 6. æ¸…ç† â”€
        if os.path.exists(wav_path):
            os.remove(wav_path)
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return result
